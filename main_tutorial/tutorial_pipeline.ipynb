{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Processing with Python -- Tutorial Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  About this Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This tutorial aims to teach the basics of (bio-)image processing with python, in particular the analysis of microscopy data in biology. It is based on an example pipeline for 2D cell segmentation and it follows a 'learning by doing' philosophy.*\n",
    "\n",
    "\n",
    "#### Instructions\n",
    "\n",
    "- This notebook contains detailed instructions on how to program a pipeline to segment cells in 2D microscopy images.\n",
    "\n",
    "\n",
    "- Simply go through the instructions step by step and try to implement each step as best as you can.\n",
    "    - By doing so, you will learn about key concepts of (bio-)image processing with python!\n",
    "\n",
    "\n",
    "- If you are stuck...\n",
    "    - ...first think some more about the problem and see if you can get yourself unstuck.\n",
    "    - ...search the internet (in particular StackOverflow) for a solution to your problem.\n",
    "    - ...if you are working through this tutorial in class, ask one of the tutors for help.\n",
    "    - ...if nothing else helps, you can have a look at the solutions (`tutorial_pipeline_solutions.py`) for inspiration.\n",
    "\n",
    "\n",
    "#### Background \n",
    "\n",
    "The aim of this pipeline is the *identification and segmentation of cells* in 2D confocal fluorescence microscopy images. Cell identification and segmentation are among the most common tasks in bio-image analysis and are often essential for the extraction of useful quantitative information from microscopy data.\n",
    "\n",
    "The pipeline is optimized to run with the provided example images, which are dual-color spinning-disc confocal micrographs (objective: 40X 1.2NA W) of cells in a live zebrafish embryo in early development (~10h post fertilization), fluorescently labeled with two membrane-localized fusion proteins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Depending on your preferences, there are several alternatives for implementing this pipeline:\n",
    "    1. Implement it directly in this Jupyter notebook *[recommended]*.\n",
    "    2. Implement it in an Integrated Development Environment (IDE) like Spyder.\n",
    "    3. Implement it in a simple text editor, running the program on the terminal.\n",
    "\n",
    "\n",
    "- Make sure that you have downloaded the **example image data** (`example_cells_1.tif` and `example_cells_2.tif`).\n",
    "\n",
    "\n",
    "- **Python version**\n",
    "    - The solutions are provided in **python 2.7.x**, which is pre-installed if you are doing this tutorial in class.\n",
    "    - However, most of the code should work with only minor changes in python 3.x, so feel free to use python 3 if you are more comfortable with it.\n",
    "\n",
    "\n",
    "- **Required modules**\n",
    "    - If you are doing this tutorial in class, all required modules are pre-installed.\n",
    "    - Otherwise, you should make sure that they are installed before you get started.\n",
    "    - List of all required modules:\n",
    "        - numpy\n",
    "        - scipy\n",
    "        - matplotlib\n",
    "        - scikit-image\n",
    "        - tifffile\n",
    "    - With the exception of tifffile, all required modules come pre-installed if you are using the **Anaconda distribution** of python.\n",
    "    - You can install tifffile from the commandline using the command `pip install tifffile` or (if you are using Anaconda) `conda install tifffile -c conda-forge`.\n",
    "\n",
    "\n",
    "\n",
    "- **Python legacy: integer division**\n",
    "    - Using python 2.7.x has a curious caveat: integer divisions that should result in fractions do not automatically convert to float type. For example, `3/2` will return `1` (preserving integer type) instead of the more sensible `1.5` (converting to float type). Fortunately, the improved division function of python 3 can be imported from \"the future\", see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "print 3/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*With this, you are ready to get started!*\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Modules & Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the package NumPy, which enables the manipulation of numerical arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_(Note: if you are not familiar with arrays and NumPy, we strongly recommend that you first complete the accompanying introductory tutorial on this topic before carrying on here.)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that once imported, we can use functions/modules from the package, for example to create an array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]\n",
      "<type 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1, 2, 3])\n",
    "\n",
    "print a\n",
    "print type(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the package is imported under a variable name (here `np`). You can choose this name freely yourself. For example, it would be just as valid (but not as convenient) to write:\n",
    "\n",
    "```python\n",
    "import numpy as lovelyArrayTool\n",
    "a = lovelyArrayTool.array([1,2,3])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='teal'> Exercise </font>\n",
    "\n",
    "Using the import command as above, follow the instructions in the comments below to import two additional modules that we will be using frequently in this pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The plotting module matplotlib.pyplot as plt\n",
    "\n",
    "# The image processing package scipy.ndimage as ndi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing & Handling Image Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Background\n",
    "\n",
    "Images are essentially just numbers (representing intensity) in an ordered grid of pixels. Image processing means simply to carry out mathematical operations on these numbers.\n",
    "\n",
    "The ideal object for storing and manipulating ordered grids of numbers is the **array**. Many mathematical operations are well defined on array and can be computed quickly by vector-based computation.\n",
    "\n",
    "Arrays can have any number of dimensions (or \"axes\"). For example, a 2D array could represent the x and y axis of a grayscale image, a 3D array could contain a z-stack (zyx), a 4D array could also have multiple channels for each image (czyx) and a 5D array could have time on top of that (tczyx)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='teal'> Exercise </font>\n",
    "\n",
    "We will now proceed to import some image data, verifying that we get what we expect and then further specifying the data we will work with. Before you start, it makes sense to have a quick look at the data in Fiji/ImageJ so you know what you are working with!\n",
    "\n",
    "Follow the instructions in the comments below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# (i) Specify the filename\n",
    "# Create a string variable with the name of the file to be imported ('example_cells_1.tif')\n",
    "# Suggested name for the variable: filename\n",
    "# Note: If the file is not in your current working directory, the filename variable must contain the \n",
    "#       entire path to the file, for example r'/home/jack/data/example_cells_1.tif'. Note the r at\n",
    "#       the beginning of the string: it designates this string as a \"raw\" string, which helps to\n",
    "#       avoid problems with slashes and other special symbols\n",
    "\n",
    "\n",
    "# (ii) Load the image\n",
    "# Import the function 'imread' from the module 'tifffile'.\n",
    "\n",
    "# Load 'example_cells_1.tif' and store it in a variable.\n",
    "# Suggested name for the variable: img\n",
    "\n",
    "\n",
    "# (iii) Check that everything is in order\n",
    "# Check that 'img' is a variable of type 'ndarray' - use Python's built-in function 'type'.\n",
    "\n",
    "# Print the shape of the array using the numpy-function 'shape'. \n",
    "# Make sure you understand the output; recall that the image has 2 color channels and is 930 by 780 pixels. \n",
    "\n",
    "# Check the datatype of the individual numbers in the array. You can use the array attribute 'dtype' to do so.\n",
    "    \n",
    "\n",
    "# (iv) Allocate the green channel to a separate new variable\n",
    "# For segmentation, we will only work with the green channel, so we need to allocate it to a new variable. \n",
    "# The green channel in this image is the first channel (or channel 0 in python). \n",
    "# We can allocated it to a new variable by slicing the 'img' array.\n",
    "# Hint: Recall that the image has three dimensions, two (rows and columns) defining the size of the image \n",
    "#       in terms of pixels, and one defining the number of channels. To slice the array, you need to index  \n",
    "#       each dimension to specify what you want from it.\n",
    "#       For example, array A below has two dimensions.\n",
    "#         A = np.array([[1,2,3],[4,5,6]])\n",
    "#       To obtain all entries in the first row, we would slice like this:\n",
    "#         B = A[0,:]\n",
    "#       You can slice the 2D green channel out of the 3D 'img' array in a similar fashion. \n",
    "\n",
    "\n",
    "# (v) Look at the image to confirm that everything worked as intended\n",
    "# Show one of the channels as an image; use pyplot's functions plt.imshow followed by plt.show. \n",
    "# Check the documentation for plt.imshow and note the parameters that can be specified, such as the color map (cmap)\n",
    "# and interpolation. Since you are working with scientific data, interpolation is unwelcome, so you should set it to\n",
    "# 'none'. The most common cmap for grayscale images is naturally 'gray'.\n",
    "# You may also want to adjust the size of the figure. You can do this by preparing the figure canvas with\n",
    "# the function plt.figure before calling plt.imshow. The canvas size is adjusted using the keyword argument\n",
    "# figsize when calling plt.figure.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of image preprocessing is to prepare or optimize the images to make further analysis easier. The specific preprocessing steps used in a pipeline depend on the type of image, the microscopy technique used, the image quality, and the desired downstream analysis. \n",
    "\n",
    "The most common operations include:\n",
    "\n",
    "- <font color='red'> What about deconvolution? </font>\n",
    "\n",
    "\n",
    "- Conversion to 8-bit images to save memory / computational time\n",
    "    - *Our images are already 8-bit*\n",
    "\n",
    "\n",
    "- Cropping of images to an interesting region\n",
    "    - *Our field of view is fine as it is*\n",
    "\n",
    "\n",
    "- Smoothing of technical noise\n",
    "    - This is a very common step and is likely to improve almost any type of downstream analysis\n",
    "    - Commonly used filters are the `Gaussian filter` and the `median filter`\n",
    "    - *Here, we will be using a Gaussian filter.*\n",
    "\n",
    "\n",
    "- Corrections of technical artifacts\n",
    "    - Common examples are uneven illumination and multi-channel bleed-through\n",
    "    - *Here, we will deal with uneven signal by adaptive thresholding*\n",
    "\n",
    "\n",
    "- Background subtraction\n",
    "    - There are various ways of sutracting background signal from an image\n",
    "    - Two different types are often distinguished:\n",
    "        - `uniform background subtraction` treats all regions of the image the same\n",
    "        - `adaptive background subtraction` automatically accounts for differences between regions of the image\n",
    "    - *Here, we do something similar to adaptive background subtraction when we do adaptive thresholding*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian Smoothing\n",
    "\n",
    "A Gaussian filter smoothens an image by convolving it with a Gaussian-shaped kernel. In the case of a 2D image, the Gaussian kernel is also 2D and can be imagined to look something like this:\n",
    "\n",
    "**<font color='red'> TODO: Maybe add self-made illustrations similar to these: </font>**\n",
    "- http://homepages.inf.ed.ac.uk/rbf/HIPR2/figs/gauss2.gif \n",
    "- http://homepages.inf.ed.ac.uk/rbf/HIPR2/figs/gausmask.gif\n",
    "- \\+ add some text explaining the kernel array image\n",
    "\n",
    "How much the image is smoothed by a Gaussian kernel is determined by the standard deviation  of the Gaussian distribution, usually referred to as **sigma** ($\\sigma$). A higher $\\sigma$ means a broader distribution and thus more smoothing.\n",
    "\n",
    "**Choosing the correct value of $\\sigma$:** this depends a lot on your images, in particular on the pixel size. In general, the chosen $\\sigma$ should be large enough to blur out noise but small enough so the \"structures of interest\" do not get blurred too much. Usually, the best value for $\\sigma$ is simply found by trying out some different options and looking at the result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='teal'> Exercise </font>\n",
    "\n",
    "Follow the instructions in the comments below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (i) Create a variable for the smoothing factor sigma, which should be an integer value\n",
    "# After implementing the Gaussian smoothing function below, you can modify this variable \n",
    "# to find the ideal value of sigma.\n",
    "\n",
    "\n",
    "# (ii) Perform the smoothing on the image\n",
    "# To do so, use the Gaussian filter function 'ndi.filters.gaussian_filter' from the \n",
    "# image processing package ndimage, which was imported at the start of the tutorial. \n",
    "# Check out the documentation of scipy to see how to implement this function. \n",
    "# Allocate the output to a new variable.\n",
    "\n",
    "\n",
    "# (iii) Visualize the result using plt.imshow and plt.show\n",
    "# Compare with the original image visualized in the step above. \n",
    "# Does the output make sense? Is this what you expected? \n",
    "# Can you optimize sigma such that the image looks smooth without blurring the membranes too much?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive Thresholding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Background\n",
    "\n",
    "The easiest way to distinguish foreground objects (here: membranes) from the image background is to threshold the image, meaning all pixels with an intensity above a certain threshold are accepted as foreground, all others are set as background. However, just applying a fixed intensity threshold often gives rather poor results due to varying background and foreground intensities across the image.\n",
    "\n",
    "One way of solving this problem is to use an *adaptive thresholding* algorithm, which adjusts the threshold locally in different regions of the image to account for varying signal-to-noise ratios. \n",
    "\n",
    "Our approach to adaptive tresholding works as follows:\n",
    "\n",
    "1. *Generation of a \"background image\":* this image should always have higher intensities than the local background but lower intensities than the local foreground. This can be achieved by strong blurring/smoothing of the image, as illustrated in this 1D example:\n",
    "\n",
    "    > <font color='red'> **ADD 1D EXAMPLE OF ADAPTIVE THRESHOLD!** </font>   \n",
    "    \n",
    "2. *Thresholding of original image with the background.* In practice, thresholding means creating a *mask*; an array of type `np.bool` which contains only the values `True` and `False` (or `1` and `0`). Any pixel with an intensity higher than the corresponding background pixel is set to `1` and all others to `0`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='teal'> Exercise </font>\n",
    "\n",
    "Implement the two steps of adaptive background subtraction:\n",
    "\n",
    "1. Use a strong \"mean filter\" to create the background image. This simply assigns each pixel the average value of its local neighborhood. Just like the Gaussian blur, it can be done by deconvolution, but this time using a \"uniform kernel\":\n",
    "\n",
    "    > <font color='red'> **ADD UNIFORM KERNEL** </font>\n",
    "    \n",
    "    > To define which pixels should be considered as the \"neighborhood\" of a given pixel, a `structuring element` (`SE`) is    used. This is a small binary image where all pixels set to `1` will be considered part of the neighborhood and all pixels set to `0` will not be considered. Here, we use a disc-shaped `SE`, as this reduces artifacts compared to a square `SE`.\n",
    "  \n",
    "    > *Side note:* The Gaussian blur above also used an `SE` but it was defined automatically by the `gaussian_filter` function based on the $\\sigma$ value we specified. Here, we explicitely generate the `SE`.    \n",
    "\n",
    "2. Use the background image for thresholding. Pixels with higher values in the original image than in the background should be given the value 1 and pixels with lower values in the original image than in the background should be given the value 0. The resulting binary image should represent the cell membranes.\n",
    "\n",
    "Follow the instructions in the comments below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 1\n",
    "\n",
    "# (i) Create a disk-shaped structuring element and asign it to a new variable.\n",
    "# Structuring elements are small binary images that indicate which pixels \n",
    "# should be considered as the 'neighborhood' of the central pixel. \n",
    "#\n",
    "# An example of a small disk-shaped SE would be this:\n",
    "#   0 0 1 0 0\n",
    "#   0 1 1 1 0\n",
    "#   1 1 1 1 1\n",
    "#   0 1 1 1 0\n",
    "#   0 0 1 0 0\n",
    "#\n",
    "# The equation below creates such structuring elements. \n",
    "# It is an elegant but complicated piece of code and at the moment it is not \n",
    "# necessary for you to understand it in detail. Use it to create structuring \n",
    "# elements of different sizes (by changing 'i') and find a way to visualize \n",
    "# the result.\n",
    "# \n",
    "# Try to answer the following questions: \n",
    "#   - Is the resulting SE really circular? \n",
    "#   - Can certain values of 'i' cause problems? If so, why?\n",
    "#   - What value of i should used for the se?\n",
    "#     Note that, similar to the sigma in Gaussian smoothing, the size of the SE\n",
    "#     is first estimated based on the images and by thinking about what would \n",
    "#     make sense. Later, it can be optimized by trial and error.\n",
    "\n",
    "#struct = np.mgrid[:i,:i][0] - np.floor(i/2)**2 + (np.mgrid[:i,:i][1] - np.floor(i/2))**2) <= np.floor(i/2)**2\n",
    "\n",
    "\n",
    "# (ii) Create the background\n",
    "# Run a mean filter over the image using the disc SE and assign the output to a new variable.\n",
    "# Use the function 'skimage.filters.rank.mean' (you first need to import the 'skimage.filters.rank' module).\n",
    "# Think about why a mean filter is used and if a different function (e.g. minimum, maximum or median) \n",
    "# would work equally well.\n",
    "\n",
    "\n",
    "# (iii) Visualize the resulting background image. \n",
    "# Compare it to the images generated above. Does the outcome make sense?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 2  \n",
    "\n",
    "# (iv) Threshold the Gaussian-smoothed original image using the background image created in step 1 \n",
    "#      to obtain the cell membrane segmentation\n",
    "# Set pixels with higher values in the original than in the bg to 1 and pixels with lower values to 0. \n",
    "# You can use a \"relational operator\" to do this, since numpy arrays will automatically perform element-wise\n",
    "# comparisons when compared to other arrays of the same shape.\n",
    "\n",
    "\n",
    "# (v) Visualize and understand the output. \n",
    "# What do you observe? \n",
    "# Are you happy with this result as a membrane segmentation? \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving Masks with Binary Morphology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Background\n",
    "\n",
    "Morphological operations such as `erosion`, `dilation`, `closing` and `opening` are common tools used (among other things) to improve masks after they are generated by thresholding. They can be used to fill small holes, remove noise, increase or decrease the size of an object, or smoothen mask outlines.\n",
    "\n",
    "Most morphological operations are - once again - simple kernel functions that are applied at each pixel of the image based on their neighborhood as defined by an `structuring element` (`SE`). For example, `dilation` simply assigns the central pixel the maximum pixel value within the neighborhood; it is a maximum filter. Conversely, `erosion` is a minimum filter. Additional options emerge from combining the two: `morphological closing`, for example, is a `dilation` followed by an `erosion`. This is used to fill in gaps and holes or smoothing mask outlines without significantly changing the mask's area. Finally, there are also some more complicated morphological operations, such as `hole filling`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='teal'> Exercise </font>\n",
    "\n",
    "Improve the membrane segmentation from above with morphological operations.\n",
    "\n",
    "Specifically, use `binary hole filling` to get rid of the speckles of foreground pixels that litter the insides of the cells. Furthermore, try different other types of morphological filtering to see how they change the image and to see if you can improve the membrane mask even more, e.g. by filling in gaps.\n",
    "\n",
    "Follow the instructions in the comments below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (i) Get rid of speckles using binary hole filling\n",
    "# Use the function ndi.binary_fill_holes for this. Be sure to read up on the docs to\n",
    "# understand exactly what it does. For this to work as intended, you will have to \n",
    "# invert the mask, which you can do using the function np.logical_not. Again, be\n",
    "# sure to understand why this has to be done.\n",
    "\n",
    "\n",
    "# (ii) Try out other morphological operations to further improve the membrane mask\n",
    "# The various operations are available in ndimage, for example ndi.binary_closing.\n",
    "# Play around and see how the different functions affect the mask. Can you optimize\n",
    "# the mask, for example by closing gaps?\n",
    "# Note that the default SE for these functions is a square. Feel free to create a\n",
    "# new disc-shaped SE and see how that changes the outcome.\n",
    "# Also, if you pay close attention you will notice that some of these operations\n",
    "# introduce artifacts at the image boundaries. Can you come up with a way of\n",
    "# solving this?\n",
    "\n",
    "\n",
    "# (iii) Visualize the final result\n",
    "# At this point you should have a pretty neat membrane mask.\n",
    "# If you are not satisfied with the quality your membrane segmentation, you should go back \n",
    "# and fine tune size of the SE in the adaptive thresholding section and also optimize the\n",
    "# morphological cleaning operations.\n",
    "# Note that the quality of the membrane segmentation will have a significant impact on the \n",
    "# cell segmentation we will perform downstream.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connected Components Labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Background\n",
    "\n",
    "Based on our membrane segmentation, we can get a preliminary segmentation of the cells in the image by considering each background region surrounded by membranes as a cell. This can already be good enough for many simple analyses.\n",
    "\n",
    "The only thing we still need to do in order to get there is to label each cell individually. Only if each separate cell has a unique number (an `ID`) assigned, we can analyze values such as the mean intensity at the single-cell level.\n",
    "\n",
    "The simple function we can use to achieve this is called `connected components labeling`. It gives every connected group of foreground pixels a unique `ID` number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='teal'> Exercise </font>\n",
    "\n",
    "Use your membrane segmentation for connected components labeling.\n",
    "\n",
    "Follow the instructions in the comments below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (i) Label connected components\n",
    "# Use the function ndi.label from ndimage. \n",
    "# Note that this function labels foreground pixels (`1`), so you may need to invert your mask\n",
    "# again if your membrane mask is currently labeled as foreground.\n",
    "\n",
    "\n",
    "# (ii) Visualize the output\n",
    "# Here, it is no longer ideal to use a 'gray' colormap, since we want to visualize that each\n",
    "# cell has a unique ID. Play around with different colormaps (check the docs to see what\n",
    "# types of colormaps are available) and choose one that you are happy with\n",
    "# Take a close look at the picture and note mistakes in the segmentation. Depending on the\n",
    "# quality of your membrane mask, there will most likely be some cells that are falsely \n",
    "# labeled as the same cells; this is called \"under-segmentation\". We will resolve this\n",
    "# issue in the next step. Note that our downstream pipeline does not involve any steps to\n",
    "# resolve \"over-segmentation\", so you should fine-tune your membrane mask such that this\n",
    "# is not a common problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell Segmentation by Seeding & Expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Background\n",
    "\n",
    "The segmentation we achieved by membrane masking and connected components labeling is a good start. We could for example use it to measure the fluorescence intensity in each cell's cytoplasm. However, we cannot use it to measure intensities at the membrane of the cells, nor can we use it to accurately measure things like cell shape or size.\n",
    "\n",
    "To improve this (and to also resolve cases of under-segmentation), we can use a \"seeding & expansion\" strategy. Expansion algorithms such as the `watershed` start \"growing outward\" from a small `seed` until they touch the boundaries of neighboring cells, which are themselves growing outward from neighoring seeds. Since the \"growth rate\" at the boundaries of the growing seeds is adjusted based on image intensity (higher intensity means slower expansion), these expansion methods often end up perfectly tracing a the cells' outlines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seeding by Distance Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Background\n",
    "\n",
    "An array of `seeds` contains a few pixels at the center of each cell labeled by a unique ID number and otherwise surrounded by zeros. The expansion algorithm will start from these central pixels and grow outward until all zeros are overwritten by an ID label. In the case of `watershed` expansion, one can imagine the `seeds` as the sources from which water pours into the cells and starts filling them up.\n",
    "\n",
    "For multi-channel images that contain a nuclear staining, it is quite common to mask the nuclei by thresholding and use e.g. an eroded version of the nuclei as seeds for cell segmentation. However, there are good alternative seeding approaches for cases where nuclei are not available or not nicely separable by thresholding.\n",
    "\n",
    "Here, we will use a `distance transform` for seeding. In a `distance transform`, each pixel in the foreground (here the cells) is assigned a value corresponding to its distance from the closest background pixel (here the membrane segmentation). In other words, we encode within the image how far each pixel of a cell is away from the membrane. The pixels furthest away from the membrane will be at the center of the cells and will have the highest values. Using a function to detect `local maxima`, we will find these high-value peaks and use them as seeds for our segmentation.\n",
    "\n",
    "One big advantage of this approach is that it will create two separate seeds for two cells, even if the two cells are connected by a hole in the membrane segmentation. Consequently, under-segmentation artifacts will be reduced.\n",
    "\n",
    "> <font color='red'> Add illustration of distance transform <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='teal'> Exercise </font>\n",
    "\n",
    "Retrieve seeds using distance transformation.\n",
    "\n",
    "This involves the following three steps:\n",
    "\n",
    "1. Run the distance transform on your membrane mask.\n",
    "\n",
    "2. Due to irregularities in the membrane shape, the distance transform may have some smaller local maxima in addition to those at the center of the cells. This will lead to additional seeds, which will lead to over-segmentation. To resolve this problem, smoothen the distance transform by applying a dilation/maximum filter. \n",
    "\n",
    "3. Find the seeds by detecting local maxima. Optimize the seeding by changing the dilation in step 2, aiming to have exactly one seed for each cell.\n",
    "\n",
    "Follow the instructions in the comments below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (i) Distance transform on thresholded membranes\n",
    "# Use the function ndi.distance_transform_edt.\n",
    "    \n",
    "    \n",
    "# (ii) Visualise the output and understand what you are seeing.\n",
    "\n",
    "\n",
    "# (iii) Dilate the distance threshold\n",
    "# Use ndi.filters.maximum_filter to dilate the distance transform.\n",
    "# Read the documentation to remind yourself how and where the structuring element can be defined with this function.\n",
    "# You can try different SE sizes and shapes. \n",
    "\n",
    "\n",
    "# (iv) Retrieve the local maxima (the 'peaks') in the distance transform\n",
    "# Use the function peak_local_max from the module skimage.feature. By default, this function will return the\n",
    "# indices of the pixels where the local maxima are. However, we instead need a boolean mask of the same shape \n",
    "# as the original image, where all the local maximum pixels are labeled as `1` and everything else as `0`.\n",
    "# This can be achieved by setting the keyword argument 'indices' to False.\n",
    "\n",
    "\n",
    "# (v) Visualize the output as an overlay on the original (smoothed) image\n",
    "# If you just look at the local maxima image, it will simply look like a bunch of distributed dots.\n",
    "# To get an idea if the seeds are well-placed, you will need to overlay these dots onto the original image.\n",
    "# To do this, it is important to first understand a key point about how the pyplot module works: \n",
    "# every plotting command is slapped on top of the previous plotting commands, until everything is ultimately \n",
    "# shown when plt.show is called. Hence, you can first plot the original input (or the smoothed) image and \n",
    "# then plot the seeds on top of it before showing both with 'plt.show'.\n",
    "# As you can see if you try this, you will not get the desired result because the zero values in seed array\n",
    "# are painted in black over the image you want in the background. To solve this problem, you need to mask \n",
    "# these zero values before plotting the seeds. You can do this by creating an appropriately masked array\n",
    "# using the function 'np.ma.array'. Check the docs to figure out how to do this.\n",
    "\n",
    "\n",
    "# (vi) Optimize the seeding\n",
    "# Ideally, there should be exactly one seed for each cell.\n",
    "# If you are not satisfied with your seeding, go back to the dilation step above and optimize it to get \n",
    "# rid of additional maxima. You can also try using the keyword argument min_distance in peak_local_max \n",
    "# to solve cases where there are multiple small seeds at the center of a cell.\n",
    "# Note that good seeding is essential for a good segmentation with an expansion algorithm. However,\n",
    "# no segmentation is ever perfect, so it's okay if a few cells end up being oversegmented!\n",
    "\n",
    "\n",
    "# (vii) Label the seeds\n",
    "# Use connected component labeling to give each cell seed a unique ID number.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expansion by Watershed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Background\n",
    "\n",
    "To achieve a cell segmentation, the `seeds` now need to be expanded outward until they follow the outline of the cell. The most commonly used expansion algorithm is the `watershed`.\n",
    "\n",
    "Imagine the intensity in the raw/smoothed image as a topographical height profile; high-intensity regions are peaks, low intensity pixels are valleys. In this representation, cells are deep valleys (with the seeds at the center), enclosed by mountains. As the name suggests, the `watershed` algorithm can be understood as the gradual filling of this landscape with water, starting from the seed. As the water level rises, the seed expands - until it final reaches the 'spine' of the cell membrane 'mountain range'. Here, the water would flow over into the neighboring valley, but since that valley is itself filled up with water from the neighboring cell's seed, the two water surfaces touch and the expansion stops.\n",
    "\n",
    "> <font color='red'> Add watershed illustration! </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='teal'> Exercise </font>\n",
    "\n",
    "Expand your seeds by means of a watershed expansion.\n",
    "\n",
    "Follow the instructions in the comments below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (i) Perform watershed\n",
    "# Use the function watershed from the module skimage.morphology.\n",
    "# Use the labeled cell seeds and the smoothed membrane image as input.\n",
    "\n",
    "\n",
    "# (ii) Show the result as transparent overlay over the smoothed input image\n",
    "# This can be done similar to the masked overlay of the seeds, but now you don't need to mask \n",
    "# the background in the overlayed image (there will be none, since everything gets labeled in\n",
    "# the watershed). Instead, you need to make the overlayed image semi-transparent. \n",
    "# This can be achieved using the optional argument 'alpha' of the 'plt.imshow' function \n",
    "# to specify the opacity.\n",
    "# Be sure to choose an appropriate colormap that allows you to distinguish the segmented\n",
    "# cells (I would suggest 'prism').\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *A Note on Segmentation Quality*\n",
    "\n",
    "This concludes the segmentation of the cells in the example image. Depending on the quality you achieved in each step along the way, the final segmentation may be of greater or lesser quality (in terms of over-/under-segmentation errors).\n",
    "\n",
    "It should be noted that the segmentation will likely *never* be 'perfect'! This can't be helped because image segmentation is ultimately a `computational classification task` and all such tasks are subject to a fundamental trade-off between specificity and sensitivity, which in this case takes the form of a trade-off between over- and under-segmentation.\n",
    "\n",
    "This raises an important question: ***when should I stop trying to optimize my segmentation?***\n",
    "\n",
    "There is no absolute answer to this question, but the best answer is probably ***when you can use it to address your biological questions.***\n",
    "\n",
    "*Importantly, this implies that you should already have a relatively clear question in mind when you are working on the segmentation!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postprocessing: Removing Cells at the Image Border"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Background\n",
    "\n",
    "Since segmentation is never perfect, it often makes sense to explicitely remove artifacts afterwards. For example, one could filter out objects that are too small, have a very strange shape, or very strange intensity values. \n",
    "\n",
    "**Warning:** Filtering out objects is equivalent to the *removal of outliers* in data analysis and *should only be done for good reason and with caution!*\n",
    "\n",
    "As an example of postprocessing, we will now filter out a particular group of problematic cells: those that are cut off at the image border."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='teal'> Exercise </font>\n",
    "\n",
    "Iterate through all the cells in your segmentation and remove those that are at the image border.\n",
    "\n",
    "Follow the instructions in the comments below. Note that the instructions will tend to be less specific from here on, so you need to figure out how to approach a problem yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (i) Create image border mask\n",
    "# We need some way to check if a cell is at the border. For this, we generate a 'mask' of the image border,\n",
    "# i.e. a Boolean array of the same size as the image where only the border pixels are set to `1` and all \n",
    "# others to `0`, like this:\n",
    "#   1 1 1 1 1\n",
    "#   1 0 0 0 1\n",
    "#   1 0 0 0 1\n",
    "#   1 0 0 0 1\n",
    "#   1 1 1 1 1\n",
    "# There are multiple ways of generating this mask, for example by erosion or by array indexing.\n",
    "# It is up to you to find a way to do it.\n",
    "\n",
    "\n",
    "# (ii) 'Delete' the cells at the border:\n",
    "# Note: When modifying a segmentation (in this case by deleting some cells), it makes sense\n",
    "#       to work on a copy of the array, not on the original. This avoids unexpected behaviors,\n",
    "#       especially within jupyter notebooks. Use the function np.copy to copy an array.\n",
    "\n",
    "# Iterate over all cells in the segmentation. Use a for-loop and the function np.unique;\n",
    "# remember that each cell in our segmentation is labeled with a different integer.\n",
    "\n",
    "    # Create a mask that contains only the 'current' cell in the iteration\n",
    "    # Hint: Remember that the comparison of an array with some number (array==number)\n",
    "    #       returns a Boolean mask of the pixels in 'array' whose value is 'number'.\n",
    "\n",
    "    # Using the cell mask and the border mask from above, test if the cell has pixels touching \n",
    "    # the image boundary or not.\n",
    "    # Hint: np.logical_and\n",
    "\n",
    "    # If a cell touches the image boundary, delete it by setting its pixels in the segmentation to 0.\n",
    "\n",
    "    \n",
    "# OPTIONAL: re-label the remaining cells to keep the numbering consistent from 1 to N (with 0 as background).\n",
    "\n",
    "\n",
    "# (iii) Visualize the result\n",
    "# Show the result as transparent overlay over the original/blurred image. \n",
    "# Here you have to combine alpha (to show cells transparently) and 'np.ma.array'\n",
    "# (to hide empty space where the border cells were deleted).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying Cell Edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Background\n",
    "\n",
    "With the final segmentation in hand, we can now start to think about measurements and data analysis. However, to extract interesting measurements from our cells, the segmentation on its own is often not enough: additional masks that identify sub-regions for each cell allow more precise and more biologically relevant measurements.\n",
    "\n",
    "The most useful example of this is an additional mask that identifies only the edge pixels of each cell. This is useful for a number of purposes, including:\n",
    "\n",
    "- Edge intensities are a good measure of membrane intensity, which is often a desired readout.\n",
    "- The intensity profile along the edge contains information on cell polarity.\n",
    "- The length of the edge (relative to the cell area) is an informative feature about the cell shape. \n",
    "- Showing colored edges can be a nice way of visualizing cell segmentations.\n",
    "\n",
    "There are many ways of identifying edge pixels in a fully labeled segmentation. Here, we will use a simple and relatively fast method based on erosion. In the <font color='green'>optional advanced content</font> you will find an even faster solution as an example of how `vectorization` can speed up your code. <font color='red'>**(this may be taken out)**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> **[** </font>\n",
    "`ndi.filters.generic_filter` is a powerful way of quickly iterating any function over numpy arrays (including functions that use a structuring element). `generic_filter` iterates a structure element over all the values in an array and passes the corresponding values to a user-defined function. The result returned by this function is then allocated to the pixel in the image that corresponds to the origin of the se. Check the documentation to find out more about the arguments for `generic_filter`.\n",
    "<font color='red'> **] Tagged for removal. Maybe add an example of generic_filter to the optional advanced content?** </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='teal'> Exercise </font>\n",
    "\n",
    "Create a labeled mask of cell edges by following these steps:\n",
    "\n",
    "\n",
    "- Create an empty array of the same size and data type as the segmentation\n",
    "    - This will be your final cell edge mask; you gradually add cell edges as you iterate over cells\n",
    "    \n",
    "\n",
    "- *For each cell:*\n",
    "    - Erode the cell's mask by 1 pixel\n",
    "    - Using the eroded mask and the original mask, create a new mask of only the cell's edge pixels\n",
    "    - Add the cell's edge pixels into the empty image generated above, labeling them with the cell's original ID number\n",
    "\n",
    "\n",
    "Follow the instructions in the comments below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (i) Create an empty array of the same size and data type as the segmentation\n",
    "# Hint: np.zeros_like(...) or np.zeros(...,dtype=...)\n",
    "\n",
    "\n",
    "# (ii) Iterate over the cells\n",
    "# Hint: np.unique\n",
    "\n",
    "\n",
    "    # (iii) Erode the cell's mask by 1 pixel\n",
    "    # Hint: smart indexing and ndi.binary_erode\n",
    "    \n",
    "    \n",
    "    # (iv) Create cell edge mask\n",
    "    # Hint: np.logical_xor\n",
    "    \n",
    "    \n",
    "    # (v) Add the cell edge mask to the empty array generated above, labeling it with the cell's ID\n",
    "    # Hint: smart indexing\n",
    "    \n",
    "\n",
    "# (vi) Visualize the result\n",
    "# Note: Because the lines are so thin (1pxl wide), they may not be displayed correctly in small images.\n",
    "#       If you wish, you can try and find a solution for this problem. One simple option is just to\n",
    "#       show a sub-region of the image so it is rendered bigger.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Quantitative Measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Background\n",
    "\n",
    "The ultimate goal of image segmentation is of course the extraction of quantitative measurements, in this case on a single-cell level. Measures of interest can be based on intensity (in different channels) or on the size and shape of the cells.\n",
    "\n",
    "To exemplify how different properties of cells can be measured, we will extraxt the following:\n",
    "\n",
    "- Cell ID (so all other measurements can be traced back to the cell that was measured)\n",
    "- Mean intensity of each cell, for each channel\n",
    "- Mean intensity at the membrane of each cell, for each channel\n",
    "- The cell area, i.e. the number of pixels that make up the cell\n",
    "- The cell outline length, i.e. the number of pixels that make up the cell edge\n",
    "\n",
    "*Note:* It makes sense to use smoothed/filtered/background-subtracted images for segmentation. When it comes to measurements, however, it's best to get back to the raw data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='teal'> Exercise </font>\n",
    "\n",
    "Extract the measurements listed above for each cell and collect them in a dictionary.\n",
    "\n",
    "Follow the instructions in the comments below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (i) Create a dictionary that contains a key-value pairing for each measurement\n",
    "# The keys should be a strings describing the type of measurement (e.g. 'green_intensity_mean') \n",
    "# and the values should be empty lists. These empty lists will be filled with the results of \n",
    "# the measurements and the dictionary will make it easy to work with this data.\n",
    "\n",
    "\n",
    "# (ii) Record the measurements for each cell\n",
    "# Iterate over the segmented cells (np.unique).\n",
    "# Inside the loop, create a mask for the current cell and use it to extract the measurements listed above. \n",
    "# Add them to the appropriate list in the dictionary using the list.append method.\n",
    "# Hint: Remember that you can get out all the values within a masked area by indexing the image \n",
    "#       with the mask. For example, np.mean(image[cell_mask]) will return the mean of all the \n",
    "#       intensity values of 'image' that are masked by 'cell_mask'.\n",
    "\n",
    "\n",
    "# (iii) Print the results and check that they make sense\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <font color='red'> YOU ARE HERE! </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Analysis & Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have collected the readouts to a dictionary you can analyse them in any way you wish. This section shows how to do basic plotting and analysis of the results, including mapping the data back onto the image (as a 'heatmap') and producing boxplots, scatterplots and a linear fit. A more in-depth example of how to couple image analysis into advanced data analysis can be found in 'data_analysis' in the 'optional_advanced_material' directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "Follow the instructions in the comments below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (i) First get yourself an overview of what you're working with by printing the results. You can print all results by iterating over the dictionary. Make sure you fully understand the data structure (the dict and the lists contained within) before you proceed.\n",
    "\n",
    "\n",
    "# (ii) Create a box plot showing the mean cell and mean membrane intensities for both channels. You first need to retrieve the data you need from the dictionary and structure it properly so you can pass it to 'plt.boxplot'. Use the 'label' keyword of 'plt.boxplot' to label the x axis with the corresponding key names. \n",
    "\n",
    "\n",
    "# (iii) Create a scatter plot of green membrane intensity over cell size. Add a linear fit to the plot to viszualize a potential correlation.\n",
    "\n",
    "    # Import the module 'stats' from 'scipy'       \n",
    "\n",
    "    # Use the function 'stats.linregress' to do a linear fit of membrane intensity vs cell size. Be sure to read the documentation to understand the output of this function.\n",
    "        \n",
    "    # Create a scatter plot of the mean green membrane intensity vs cell size. Use the function 'plt.scatter' to do so. Remember to label the axes, using the functions 'plt.xlabel' and 'plt.ylabel'. \n",
    "\n",
    "    # Define the equation of the line that fits the data. Recall that the equation of a straight line in 2D is y = m * x + c, where m is the slope and c is the intersection of the y-axis. You need to create a function that takes in a value (x) and returns the fitted value (y). You can define your function normally (using the 'def' keyword') or if you are feeling fancy you can create an anonymous function using 'lambda'. The example below shows the difference between a normal function and an anonymous function:\n",
    "\n",
    "         # # normal function definition\n",
    "         # \tdef f(x): \n",
    "         #      \"\"\"Returns the square value\"\"\"\n",
    "         #      return x**2\n",
    "         #\n",
    "         # anonymous function\n",
    "         # f = lambda x: x**2       # returns the square value\n",
    "\n",
    "    # Next, you need to get the actual fitted values to plot your line. You only need to evaluate your function at two points at the limits of your scatter plot so you can draw the line across the entire plot. You can get the limits for the x axis using 'plt.gca' and then 'get_xlim'. Once you have these x values, you can get the corresponding fitted y values using your line function (if you used 'lambda' to define it, use 'map' to get the result).\n",
    "\n",
    "    # Plot the line         \n",
    "        # Plot the line using 'plt.plot'. You can specify the aesthetics of the line, e.g. line width, color, and so forth - see the documentation.\n",
    "        \n",
    "    # Finally, use 'plt.show' to show the plot.\n",
    "\n",
    "    \n",
    "    \n",
    "# (iv) Print out results from the linear regression (again, check the documentation of 'stats.linregress'):\n",
    "\n",
    "    # Define a list with the names of the output of the linear regression (e.g. 'slope',...)\n",
    "\n",
    "    # Iterate over all values in the 'stats.linregress' output.\n",
    "\n",
    "        # Print the output value with the corresponding name. To simultaneously iterate over the output of 'stats.linregress' and over your list of output names, the function 'enumerate' is very helpful. You can check the docs or the web to find out how to use 'enumerate' in for-loops.\n",
    "\n",
    "    # (After the for loop), also calculate and print the r-squared value\n",
    " \n",
    "    # Note that this seems to return a highly significant p-value but a very low correlation coefficient (r-value). We also would not expect this correlation to be present in our data. This should prompt several considerations:\n",
    "        # 1) What does this p-value actually mean? See help(stats.linregress)\n",
    "        # 2) Could there be artefacts in our segmentation that could bias this analysis?\n",
    "        # 3) We're now working with a lot of datapoints - this can skew statistical analyses. We can (and should!) accommodate this by multiple testing correction and by comparison with randomized datasets.\n",
    "    # In general, it's always good to be very careful when doing data analysis. Make sure you understand the functions you are using and always check for possible errors or biases in your analysis!\n",
    "\n",
    "\n",
    "# (v) Map the cell size back onto the image as a 'heatmap':\n",
    "\n",
    "    # Scale the cells sizes to 8bit (since we need them as pixel intensity values). \n",
    "    # Hint: if the largest cell size should correspond to the value 255 in uint8, then the other cell sizes correspond to cell_size*255/largest_cell_size.\n",
    "    \n",
    "    # Initialize a new image; all values should be zeros, the size should be identical to e.g. 'green_ws' and the dtype should be uint8. (Suggested name for variable: 'size_img')\n",
    "    \n",
    "    # Iterate over the segmented cells of 'green_ws'.(You need a for-loop, and the functions 'enumerate' and 'np.unique'.)\n",
    "    \n",
    "        # Assign to each pixel of 'size_img' the cell size (in uint8) of the cell it corresponds to in 'green_ws'\n",
    "    \n",
    "    # Visualize the result as a colored semi-transparent overlay over the smoothed input image 'green_smooth'.\n",
    "\n",
    "# (vi)\n",
    "# Do an analysis of statistical tests and the rest of the measurements. Think about what the results actually mean and whether any inconsistancies can be traced back to a not sufficiently good segmentation, e.g. we  have filtered for some artifacts, such as incomplete cells (that touch the boundary), but not for others; can you think of some?.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Output to Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several ways of saving the output of a program. Data can be saved to files in a human-readable format such as text files (e.g. to import into Excel), in a format readable for other programs such as tif-images (e.g. to visualize in Fiji) or in a language-specific file that makes it easy to reload the data into python in the future (e.g. for further analysis)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "Follow the instructions in the comments below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (i) Write an image to a tif (could be opened e.g. in Fiji):\n",
    "\n",
    "    # Get the file handling function 'imsave' from the package 'tifffile'\n",
    "    \n",
    "    # Save one of the images you've generated using 'imsave' and open it in Fiji\n",
    "\n",
    "\n",
    "# (ii) Write a figure to a png or pdf:\n",
    "\n",
    "    # Recreate the scatter plot from above (without the regression line). Remember to label axes.\n",
    "    \n",
    "    # Save the figure to png using 'plt.savefig'\n",
    "    \n",
    "    # Save the figure to pdf using 'plt.savefig'. This creates a vector graphic that can be imported to illustrator etc.\n",
    "\n",
    "\n",
    "\n",
    "# ***For the next two exercises, refer to the python documentation for input and output.***\n",
    "\n",
    "# (iii) Write a python file that can be reloaded in other Python programs:\n",
    "\n",
    "    # Import the module 'json'\n",
    "    \n",
    "    # Open an empty file object using 'open' in write ('w') mode. Use a name that contains the extension '.json'. You should use the 'with'-statement (context manager) to make sure that the file object will be closed automatically when you are done with it.\n",
    "    \n",
    "    # Use the function 'json.dump' to write the results.    \n",
    "    \n",
    "\n",
    "# Note: This file could be re-loaded again as follows:\n",
    "#with open(filename+'_resultsDict.json', 'r') as fp:\n",
    "#   results = json.load(fp)\n",
    "\n",
    "\n",
    "# (iv) Write a text file of the numerical data gathered (could be opened e.g. in Excel):\n",
    "\n",
    "# Open an empty file object using 'open' in write ('w') mode. Use a name that contains the extension '.txt'. You should use the 'with'-statement (context manager) to make sure that the file object will be closed automatically when you are done with it.\n",
    "    \n",
    "    # Write the headers of the data (the result dictionary keys), separated with tabs ('\\t'). You will need the function 'file.write' to write strings to the file. It makes sense to first generate a complete string of all the headers with a loop and then write it to the file.\n",
    "    \n",
    "    # Iterate over all cells saved in your results variable using a for-loop and the function 'enumerate'.\n",
    "    \n",
    "        # For each key in the dict, write the data to the text file, separated with tabs ('\\t').\n",
    "    \n",
    "    # After writing the data, have a look at the output file in a text editor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes the tutorial."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
